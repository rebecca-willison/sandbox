{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incomplete-childhood",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "injured-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-canal",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "uniform-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/rebeccawillison/Documents/research/stat-9340/hw5/mnist0_train_b.txt'\n",
    "mnist0 = np.array(pd.read_fwf(data_path, header = None)).transpose()\n",
    "data_path = '/Users/rebeccawillison/Documents/research/stat-9340/hw5/mnist9_train_b.txt'\n",
    "mnist9 = np.array(pd.read_fwf(data_path, header = None)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "strong-playlist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: (784, 2000)\n",
      "Test data size: (784, 1000)\n"
     ]
    }
   ],
   "source": [
    "# subset training dataset & standardize\n",
    "train_X = np.c_[mnist0[:,0:1000], mnist9[:,0:1000]]#.transpose()\n",
    "train_X = (train_X - np.mean(train_X))/np.std(train_X)\n",
    "train_Y = np.append(np.full(1000, 0), np.full(1000, 1))\n",
    "\n",
    "# create test dataset & standardize\n",
    "test_X = np.c_[mnist0[:,2500:3000], mnist9[:,2500:3000]]#.transpose()\n",
    "test_X = (test_X - np.mean(test_X))/np.std(test_X)\n",
    "test_Y = np.append(np.full(500, 0), np.full(500, 1))\n",
    "\n",
    "print(\"Training data size:\", train_X.shape)\n",
    "print(\"Test data size:\", test_X.shape)\n",
    "\n",
    "# convert data to tensor\n",
    "X_train = torch.from_numpy(train_X.transpose())\n",
    "Y_train = torch.from_numpy(train_Y)\n",
    "X_test = torch.from_numpy(test_X.transpose())\n",
    "Y_test = torch.from_numpy(test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-correlation",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "revised-failing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedforward(\n",
      "  (stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=40, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=40, out_features=40, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=40, out_features=2, bias=True)\n",
      "    (5): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.stack = nn.Sequential(\n",
    "            torch.nn.Linear(self.input_size, self.hidden_size),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(self.hidden_size, 2),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.stack(x)\n",
    "        return output\n",
    "\n",
    "model = Feedforward(input_size = 784, hidden_size = 40)\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, \n",
    "                            weight_decay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-blond",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incorporated-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "epoch = 20\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train.float())\n",
    "    # Compute Loss\n",
    "    loss = loss_fn(y_pred.squeeze(), Y_train)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-accent",
   "metadata": {},
   "source": [
    "### Get test predictions and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cheap-delhi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.722\n"
     ]
    }
   ],
   "source": [
    "y_test = model(X_test.float())\n",
    "test_pred = y_test[:,1].detach().numpy()\n",
    "test_pred[test_pred > np.mean(test_pred)] = 1\n",
    "test_pred[test_pred != 1] = 0\n",
    "print('Accuracy:', sum(test_pred == test_Y)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-booking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
